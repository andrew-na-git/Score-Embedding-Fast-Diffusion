{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import torch.multiprocessing as multiprocessing\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import scipy.stats as stats\n",
    "\n",
    "from utils.kfp import get_B_block, diffusion_coeff, solve_pde, get_sparse_A_block, marginal_prob_std\n",
    "from network.network import ScoreNet\n",
    "\n",
    "from data.Dataset import CIFARDataset\n",
    "\n",
    "torch.manual_seed(2);\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change these\n",
    "N = 200\n",
    "H = 28\n",
    "W = 28\n",
    "epochs = 3000\n",
    "sigma = 250\n",
    "lr = 1e-3\n",
    "###\n",
    "\n",
    "eps = 1e-6\n",
    "dt = 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFARDataset(W, H);\n",
    "\n",
    "channels = dataset.channels\n",
    "n_data = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create memory buffers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((N, channels, H*W), dtype=np.float32)\n",
    "m_prev = np.ones((N, channels, H*W), dtype=np.float32)\n",
    "scores = np.zeros((N, channels, H*W), dtype=np.float32) # initial scores guess\n",
    "dm = np.zeros_like(scores, dtype=np.float32)\n",
    "\n",
    "# we want to sample from random time steps to construct training samples\n",
    "time_ = np.linspace(eps, 1, N).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Defining pde diffusion for multi-threading\n",
    "def diffuse(x, m, dm, channel, time_, g, scores):\n",
    "\n",
    "  y_train = []\n",
    "  for j in range(H):\n",
    "    y_train.append(x[channel][j, :])\n",
    "  y_train = np.concatenate(y_train)\n",
    "  x_train = []\n",
    "  for l in range(W):\n",
    "    x_train.append(x[channel][:, l])\n",
    "  x_train = np.concatenate(x_train)\n",
    "\n",
    "  xy_train = np.vstack([x_train, y_train])\n",
    "  kde_kernel = stats.gaussian_kde(xy_train)\n",
    "  m[0, channel] = kde_kernel.logpdf(xy_train)\n",
    "  dx = 256/H*W\n",
    "  \n",
    "  sparse_A_block = get_sparse_A_block(dx, dt, g(time_[1:]), scores[1:, channel], H, W, N)\n",
    "\n",
    "  B_block = get_B_block(dx, dt, m, channel, H, W, N)\n",
    "\n",
    "  m[1:, channel] = solve_pde(sparse_A_block, B_block, mode='sp_sparse').reshape((-1, H*W))\n",
    "  img_log_prob = m[:, channel]\n",
    "  for i in range(N):\n",
    "    dm[i, channel, 1:-1] = (img_log_prob[i,:-2] - img_log_prob[i,2:])/(2*dx)\n",
    "    dm[i, channel, 0] = dm[i, channel, 1]\n",
    "    dm[i, channel, -1] = dm[i, channel, -2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 0: 588.0607299804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 1: 187.83935546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 2: 0.7760848999023438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 3: 0.005180760752409697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 4: 3.15093930112198e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual at iteration 5: 9.441224051442987e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = 1\n",
    "e = 0\n",
    "tol = 1e-6\n",
    "max_iter = 1000\n",
    "\n",
    "while res > tol:\n",
    "  for idx, data in tqdm(enumerate(dataset)):\n",
    "    \n",
    "    # diffuse all three channels concurrently\n",
    "    for ch in range(channels):\n",
    "        diffuse(data, m, dm, ch, time_, diffusion_coeff_fn, scores)\n",
    "\n",
    "    scores = dm.copy()\n",
    "\n",
    "    if e == max_iter:\n",
    "      print(f'No convergence')\n",
    "      break\n",
    "\n",
    "    res = np.linalg.norm(m - m_prev)\n",
    "    print(f'residual at iteration {e}: {res}')\n",
    "\n",
    "    m_prev = m.copy()\n",
    "    e += 1\n",
    "\n",
    "scores_label = scores.copy().reshape((-1, channels, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(model, x, label, diffusion_coeff, marginal_prob_std, eps=1e-5):\n",
    "  \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model instance that represents a\n",
    "      time-dependent score-based model.\n",
    "    x: A mini-batch of training data.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    eps: A tolerance value for numerical stability.\n",
    "  \"\"\"\n",
    "  random_t = torch.tensor(np.sort(np.random.uniform(eps, 1., N)).astype(np.float32))\n",
    "  # we encode the label into the initial data using the reverse ODE\n",
    "  diff_std2 = diffusion_coeff(2 * random_t)\n",
    "  for i in range(1, N):\n",
    "    x[i] = x[i-1] - 0.5 * label[i-1] * diff_std2[i-1] * dt\n",
    "  std = marginal_prob_std(random_t)\n",
    "  z = torch.randn_like(x)\n",
    "  # we perturb the image by the forward SDE conditional distribution\n",
    "  perturbed_x = x + z * std[:, None, None, None]\n",
    "  score = model(perturbed_x, random_t)\n",
    "  # loss = torch.mean(torch.sum((score * std[:, None, None, None] - label)**2, dim=(1, 2, 3)) / (2 * diff_std2))\n",
    "  loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1, 2, 3))) # original loss from tutorial\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3000 [00:05<50:25,  1.01s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(channels):\n\u001b[1;32m     46\u001b[0m   init_x[:, ch] \u001b[38;5;241m=\u001b[39m data[ch]\n\u001b[0;32m---> 48\u001b[0m \u001b[43mdiffuse_train_all_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_coeff_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarginal_prob_std_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# # train all three channels concurrently\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# for ch in range(channels):\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#   processes[ch] = multiprocessing.Process(target=diffuse_train, args=[ch, init_x, epochs, diffusion_coeff_fn, marginal_prob_std_fn, scores_label])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# for p in processes:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#   processes.join()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mdiffuse_train_all_channels\u001b[0;34m(init_x, epoch, diffusion_coeff, marginal_prob_std, label)\u001b[0m\n\u001b[1;32m     29\u001b[0m   loss \u001b[38;5;241m=\u001b[39m loss_fn(model_score, init_x, scores_label, diffusion_coeff, marginal_prob_std)\n\u001b[1;32m     30\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m   \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mloss at all channels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/src/Stable-Diffusion/.venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/Stable-Diffusion/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Function for training on a thread\n",
    "def diffuse_train(channel, init_x, epoch, diffusion_coeff, marginal_prob_std, label):\n",
    "  \n",
    "  model_score = ScoreNet(marginal_prob_std=marginal_prob_std)\n",
    "  optimizer = Adam(model_score.parameters(), lr=lr)\n",
    "  model_score.train();\n",
    "\n",
    "  scores_label = torch.tensor(label)[:, channel][:, None]\n",
    "  for e in tqdm(range(epoch)):\n",
    "    loss = loss_fn(model_score, init_x[:, channel][:, None], scores_label, diffusion_coeff, marginal_prob_std)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'\\nloss at channel {channel}: {loss}')\n",
    "  file = f'model_cifar_thread_{channel}.pth'\n",
    "  torch.save(model_score.state_dict(), file)\n",
    "  print(f\"model for thread {channel} has been saved\\n\")\n",
    "  \n",
    "#@title Function for training on a thread\n",
    "def diffuse_train_all_channels(init_x, epoch, diffusion_coeff, marginal_prob_std, label):\n",
    "  \n",
    "  model_score = ScoreNet(marginal_prob_std=marginal_prob_std)\n",
    "  optimizer = Adam(model_score.parameters(), lr=lr)\n",
    "  model_score.train();\n",
    "\n",
    "  scores_label = torch.tensor(label)\n",
    "  for e in tqdm(range(epoch)):\n",
    "    loss = loss_fn(model_score, init_x, scores_label, diffusion_coeff, marginal_prob_std)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'\\nloss at all channels: {loss}')\n",
    "  file = f'model_cifar_thread_all.pth'\n",
    "  torch.save(model_score.state_dict(), file)\n",
    "  print(f\"model for all channels has been saved\\n\")\n",
    "\n",
    "#processes = [None] * channels\n",
    "init_x = torch.zeros((N, channels, H, W))\n",
    "\n",
    "diffuse_train_fn = functools.partial(diffuse_train, init_x=init_x, epoch=epochs, diffusion_coeff = diffusion_coeff_fn, marginal_prob_std = marginal_prob_std_fn, label=scores_label)\n",
    "\n",
    "for idx, data in enumerate(dataset):\n",
    "  for ch in range(channels):\n",
    "    init_x[:, ch] = data[ch]\n",
    "    \n",
    "  diffuse_train_all_channels(init_x, epochs, diffusion_coeff_fn, marginal_prob_std_fn, scores_label)\n",
    "\n",
    "  # # train all three channels concurrently\n",
    "  # for ch in range(channels):\n",
    "  #   processes[ch] = multiprocessing.Process(target=diffuse_train, args=[ch, init_x, epochs, diffusion_coeff_fn, marginal_prob_std_fn, scores_label])\n",
    "  #   processes[ch].start()\n",
    "\n",
    "  # for p in processes:\n",
    "  #   processes.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
